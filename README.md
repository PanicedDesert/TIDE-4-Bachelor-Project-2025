# **avis â€” Analyzing Evolving Stories in News Articles (Refactored)**

> **Research-driven NLP + Optimization for Story Evolution**

---
## ðŸ”¹ Abstract (from the paper)
> There is an overwhelming number of news articles published every day. Tracking how a news story *evolves over time* is difficult because similarity-based methods tend to circle around the same event instead of revealing its historical origins. This project implements and extends a framework that **mines historical news to detect the origin of events, segments timelines into coherent phases, and identifies the most relevant documents at each turning point**. The approach combines NLP preprocessing, topic modeling, and a continuous optimization formulation that balances **coherence, diffusion, temporal structure, and document relevance**. Quantitative metrics and human evaluations show that the method discovers statistically significant and meaningful storylines in reasonable time, with potential for predicting future entities in evolving stories. 

*(Summarized from Barranco et al., 2017)*

---
## ðŸŽ¯ What this project is about (for recruiters)

This repository is a **clean, production-style refactor** of a research prototype for:

- **News story evolution analysis**
- **Entity-centric NLP and topic modeling (LDA, TFâ€“IDF)**
- **Graph/optimization-based storytelling (â€œconnecting the dotsâ€)**
- **Continuous-time segmentation of document streams**
- **Relevance-weighted document selection**
- **Statistical validation and human-in-the-loop evaluation**
- **Downstream prediction of future entities from past story evolution**

**Keywords:** NLP, text mining, topic modeling, constrained optimization, temporal modeling, document networks, L-BFGS-B, diffusion processes, information retrieval, explainable AI, data science.

---
## ðŸ§  Conceptual Methodology (high-level)

### 1ï¸âƒ£ Preprocessing (Framework)
- Named-entity extraction (persons, organizations, locations)
- TFâ€“IDF representation over entities
- LDA topic modeling to obtain document-topic distributions
- Temporal + topical filtering of candidate documents

### 2ï¸âƒ£ Story Generation (Core Contribution)
The system identifies **turning points in time** and assigns documents to smooth temporal segments using a continuous membership function. The objective jointly balances:

- **Incoherence (within segments)** â€” documents in the same phase should be similar
- **Unconnectedness (across segments)** â€” different phases should represent different events
- **Temporal penalty** â€” discourages grouping far-apart documents
- **Overlap penalty** â€” prevents turning points from collapsing together
- **Relevance weights** â€” highlights the most important documents per segment
- **Uniformity penalty** â€” avoids trivial solutions

Optimization is performed with **L-BFGS-B**.

---
## ðŸ“Š Figures from the paper (added visuals)

### ðŸ”¹ Diffusion vs Similarity â€” long vs short histories
![Diffusion vs Similarity](lÃ¦ngerehist.png)

This figure illustrates the core intuition of the project: diffusion-based storytelling can trace **longer, more semantically coherent historical chains**, while pure similarity tends to stay local and repetitive.

---
### ðŸ”¹ Beam Search Procedure
![Beam Search](beamSearch233.drawio.png)

This diagram shows how candidate story paths are expanded and pruned during **beam search**, balancing exploration of alternatives with computational tractability.

---
### ðŸ”¹ Statistical Effect Size (Cohenâ€™s d)
![Cohen's d](Example_cohens_d.png)

Effect size analysis demonstrates that improvements over baselines are not only statistically significant but also **practically meaningful**.

---
### ðŸ”¹ Ablation / Sensitivity Plot
![Ablation Plot](abplot.png)

This plot summarizes how sensitive the solution is to key hyperparameters (e.g., distance threshold, overlap penalty, topic divergence). Stable performance across settings indicates a robust method.

---
## ðŸ— Repository structure (clean, maintainable)
```
avis-refactored/
â”œâ”€â”€ src/avis/
â”‚   â”œâ”€â”€ nlp/           # tokenization, stopwords, preprocessing
â”‚   â”œâ”€â”€ models/        # TFâ€“IDF, KMeans topic tools
â”‚   â”œâ”€â”€ experiments/   # beam search & parameter sweep scaffolding
â”‚   â””â”€â”€ data/          # Danish stopwords
â”œâ”€â”€ original/          # your original notebooks (untouched)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

---
## ðŸš€ Installation
```bash
pip install -e .
```

## ðŸ§ª Minimal example (TFâ€“IDF + KMeans topics)
```python
from avis.nlp.stopwords import load_danish_stopwords
from avis.models.vectorize_tfidf import fit_tfidf, TfidfConfig
from avis.models.kmeans_topics import fit_kmeans_topics, top_terms_per_cluster, KMeansTopicConfig

stop = load_danish_stopwords()
docs = [
    "Dette er en artikel om politik og Ã¸konomi...",
    "Sport og fodbold nyheder...",
]

vectorizer, X = fit_tfidf(docs, stop, TfidfConfig(max_features=5000))
model = fit_kmeans_topics(X, KMeansTopicConfig(n_clusters=2))
print(top_terms_per_cluster(model, vectorizer, top_n=8))
```

---
## ðŸ Conclusion (from the paper)
> The framework successfully uncovers the historical evolution of news stories from large archives. It not only reconstructs meaningful timelines but also enables **future entity prediction** from past diffusion patterns. The authors propose extending the work toward early-warning systems for emerging events and incorporating **interactive user feedback** to adapt the optimization to human expectations.

---
## ðŸ“š Reference
Barranco, R. C., Boedihardjo, A. P., & Hossain, M. S. (2017). *Analyzing Evolving Stories in News Articles*. ACM Conference.

